---
title: 'This is the Title asdfa. '
meta_title: This is meta title
description: This is meta description
image: /images/avatar.png
draft: false
---

I attended the first day of Google I/O 2025 and left feeling a mix of excitement and anxiety. On one hand, as a user and developer, I’m excited for the new products and features. Google is truly a marvel of modern technology and that was on full display with products like Flow, AndroidXR, and Search. On the other hand, I’m terrified at what it means for the SEO community because the skillset and technology we use to support driving visibility is not prepared for where things are headed. To top it off, the ongoing conversation is keeping people complacent which is dangerous for the advancement of the field.

There’s been much chatter within the SEO community lately about how the generative AI driven features of Google make no difference; “it’s just SEO.” In fact, Google’s [latest](https://developers.google.com/search/blog/2025/05/succeeding-in-ai-search) [attempts](https://developers.google.com/search/docs/appearance/ai-features) at guidance reflect that. Much of the argument is rooted in the overlapping mechanics between generative information retrieval and classic information retrieval for the web.

Yes, you still need to make content accessible, indexable, and understood, but the difference is that in classic IR, your content comes out the same way it goes in. In generative IR, your content is manipulated and you don’t know how or if it will appear on the other side even if you did all your SEO best practices right and it informed the response. Therein lies the disconnect and the layer where SEO as it currently exists is not enough.

## The Overlaps with Classic Organic Search will be Short-Lived

Last month at [SEO Week](https://seoweek.org/), in my Brave New World of SEO talk, I doubled down by saying that, sure, there is high overlap between the organic SERPs and AI Overviews right now, but we’re not ready for what happens when memory, personalization, MCP, and the requisite agentic capabilities are mixed in. What happens when Google is pulling data from every application on the web?

With the announcement of enhancements to AI Mode, literally everything I said is either now live in your Google Search experience or on the way this year. Google has also been warning us since the launch of their [AIO and AI Mode explainer doc](https://search.google/pdf/google-about-AI-overviews-AI-Mode.pdf) that the best of AI Mode will ultimately make its way to the core search experience. The more I’ve researched how these features work, the more adamant I’ve become that our space needs to think bigger.

So, let’s talk about why we’re not ready and what we need to do to get ready.

## It’s Not Just SEO, but what is SEO Anyway?

The argument that AI Mode and AI Overviews are “just SEO” is short-sighted at best and dangerously misinformed at worst.

What this position gets wrong isn’t just technical nuance; it’s the complete misunderstanding of how these generative surfaces fundamentally differ from the retrieval paradigm that SEO was built on. The underlying assumption is that everything you’d do to show up in AI Mode is already covered by SEO best practices. But if that were true, the industry would already be embedding content at the passage level, running semantic similarity calculations against query vectors, and optimizing for citation likelihood across latent synthetic queries. The shocking lack of mainstream SEO tools that do any of that is a direct reflection of the fact that most of the SEO space is not doing what is required. Instead, our space is doing what it has always done, and sometimes it’s working.

### SEO is a Discipline Without Boundaries

Part of the confusion stems from the fact that SEO has no fixed perimeter. It has absorbed, borrowed, and repurposed concepts from disciplines like performance engineering, information architecture, UX, analytics, and content strategy, often at Google’s prompting. 

Structured data? Now SEO. Site speed? SEO. Entity modeling? SEO. And the list goes on.

In truth, if every team accounted for Google’s requirements in their own practice areas, SEO as a standalone discipline would not exist.

So what we call SEO today is more of a reactive scaffolding. It’s a temporary organizational response to Google’s structural influence on the web. And that scaffolding is now cracking under the weight of a fundamentally different paradigm: generative, reasoning-driven retrieval and the competition that has arisen on the back of it.

### SEO is Not Optimizing for AI Mode

There is a profound disconnect between what’s technically required to succeed in generative IR and what the SEO industry currently does. Most SEO software still operates on sparse retrieval models (TF-IDF, BM25) rather than dense retrieval models (vector embeddings). We don’t have tools that parse and embed content passages. Our industry doesn’t widely analyze or cluster candidate documents in vector space. We don’t measure our content’s relevance across the synthetic query set that’s never visible to us. We don’t know how often we’re cited in these generative surfaces, how prominently, or what intent class triggered the citation. The major tools have recently begun sharing rankings data for AIOs, but miss out on the bulk of them because they track based on logged-out states.

The only part that is “just SEO” is the fact that whatever is being done is being done incorrectly.

AI Mode introduces:

* Reasoning models that generate answers from multiple semantically-related documents.
* Fan-out queries that rewrite the search experience as a latent multi-query event.
* Passage-level retrieval instead of page-level indexing.
* Personalization through user embeddings, meaning every user sees something different, even for the same query in the same location.
* Zero-click behavior, where being cited matters more than being clicked.

These are not edge cases. This is the system.

So no, this is not just SEO. It’s what comes after SEO.

If we keep pretending the old tools and old mindsets are sufficient, we won’t just be invisible in AI Mode, we’ll be irrelevant.

That said, SEO has always struggled with the distinction between strategy and tactics, so it doesn’t surprise me that this is the reaction from so many folks. It’s also the type of reaction that suggests a certain level of cognitive dissonance is at play. Knowing how the technology works, I find it difficult to understand that position because the undeniable reality is that aspects of search are fundamentally different and much more difficult to manipulate.

#### Google’s Solving for Delphic Costs, Not Driving Traffic

We are no longer aligned with what Google is trying to accomplish. We want visibility and traffic. Google wants to help people meet their information needs and they look at traffic as a [“necessary evil.”](https://searchengineland.com/google-traffic-publishers-necessary-evil-453562)

Watch the search section of the Google I/O 2025 keynote or [read Liz Reid’s blog post on the same](https://blog.google/products/search/google-search-ai-mode-update/). It’s clear that they want to [do the Googling for you](https://blog.google/products/search/generative-ai-google-search-may-2024). 

On another panel, Liz explained how, historically, for a multi-part query, the user would have to search for each component query and stitch the information together themselves. This speaks to the same concepts that Andrei Broder highlights in his [Delphic Costs paper](https://arxiv.org/abs/2308.07525) on how the cognitive load for search is too high. Now, Google can pull from results from many queries and stitch together a robust and intelligent response for you.

Yes, the base level of the SEO work involved is still about being crawled, rendered, processed, indexed, ranked, and re-ranked. However, that’s just where things start for a surface like AI Mode. What’s different is that we don’t have much control over how we show up on the other side of the result. 

Google’s AI Mode incorporates reasoning, personal context, and later may incorporate aspects of DeepSearch. These are all mechanisms that we don’t and likely won’t have visibility into that make search probabilistic. The SEO community currently does not have data to indicate performance, nor tooling to support our understanding of what to do. So, while we can build sites that are technically sound, create content, and build all the links, this is just one set of many inputs that go into a bigger mix and come out unrecognizable on the other side.

SEO currently does not have enough control to encourage rankings in a reasoning-driven environment. Reasoning means that Gemini is making a series of inferences based on the historical conversational context (memory) with the user. Then there’s the layer of personal context wherein Google will be pulling in data across the Google ecosystems, starting with Gmail, MCP, and A2A man this is a platform shift and much more external context will be considered. DeepSearch is effectively an expansion of the DeepResearch paradigm brought to the SERP, where hundreds of queries may be triggered and thousands of documents reviewed.

#### The Multimodal Future of Search

Another fundamental change is that AI Mode is also natively multimodal, which means that it can pull in video, audio, and their transcripts or imagery. There’s also the aspects of [Multitask Unified Model (MUM)](https://blog.google/products/search/introducing-mum/) that underpin this, which can allow content in one language to be translated into another and used as part of the response. In other words, every response is a highly opaque matrixed event rather than the examination of a few hundred text documents based on deterministic factors.

Historically, your competitive analysis compared text-to-text in the same language or video-to-video. Now you’re dealing with a highly dynamic set of inputs, and you may not have the ability to compete.

Google’s guidance is encouraging people to invest in more varied content formats at the same time that they are [cutting people’s clicks by 34.5%.](https://ahrefs.com/blog/ai-overviews-reduce-clicks/) It will certainly be an uphill battle convincing organizations to commit these resources, especially when “non-commodity” content won’t have a long life span either. Google is [bringing custom data visualization to the SERP](https://blog.google/products/search/google-search-ai-mode-update/#custom-charts) based on your data. I can’t imagine remixing your content on the fly with Veo and Imagen are far behind. That alone changes the complexion of what we’re able to strategically accomplish in the context of an organization.

#### The Current Model of SEO Does Not Support Where Things are Going

I went to sleep the first night of I/O thinking about how futile it will be to log in to much of the SEO software we subscribe to for AI Mode work. It’s pretty clear that, at some point, Google will make AI Mode the default, and much of the SEO community won’t know what to do.

We are in a space where rankings have been highly personalized for twenty years, and still, the best we can do is rank tracking based on a hypothetical user who joined the web for the first time, and their first act is to search for your query. We’re operating on a system that has been semantic for at least ten years and hybrid for at least five, but the best we can do is lexical-based content optimization tools?

Siiiiigh…..there is a lot of work to be done.

#### Maybe James Cadwallader was Right After All

At SEO Week, James Cadwallader, co-founder and CEO of [conversational search analytics platform Profound](https://www.tryprofound.com/) casually declared that “SEO will become an antiquated function.” He quickly couched that by saying that Agent Experience (AX) is something that SEOs are uniquely positioned to transition to. 

Before he got there, he thoughtfully made his case, explaining that the original paradigm of the web was a two-sided marketplace and the advent of the agentic web upends the user-website interaction model. Poignantly, James concluded that the user doesn’t care where content comes from as long as they get viable answers.

So, while Google has historically warned us against marketing to bots, the new environment basically requires that we consider bots as a primary consumer because the bots are the interpreters of information for the end user. In other words, his thesis suggests that very soon users won’t see your website at all. Agents will tailor the information based on their understanding of the user and their reasoning against your message.

On the technical end, James talked through his team’s hypothesis on how long-term memory works. It sounds as though there’s a representation of all conversations that is constantly updated and added to the system prompt. Presumably, this is some sort of aggregated embedding or another version of the long-term memory store that further informs downstream conversations. As we’ll discuss a few hundred words from here, this aligns with the approach described in Google’s patents.

Initially, I thought his conclusions were a bit alarmist, albeit great positioning for their software. Nevertheless, one of the things that I love about Profound is that they are technologists and not beholden to the baggage of the SEO industry. They didn’t live through Florida, Panda, Penguin, or the industry uproar against Featured Snippets. They are clear-eyed consumers of what is and what will be. They operate in the way best-in-class tech companies do, so they are focused on the state of the art and shipping product quickly. Since the I/O keynote, I’ve come to recognize James is right, unless we do something!

James’s talk is more biased towards OpenAI’s offerings, but as we’ve seen, Google is going in an overlapping direction, so I definitely recommend checking it out to give context.

## No Data and No Real Direction from Google

At I/O, we had some discussions with Google engineers, and part of the conversation revolved around recognition that the relationship between them and our community is symbiotic, although simultaneously and paradoxically one-sided. After all, the web would not have adopted the secure protocol, structured data, or Core Web Vitals as fast or as completely as it did if our community did not do the legwork to make it happen. I hope whoever had those social engineering OKRs got promoted.

We also discussed how sites are losing clicks due to AIOs, and how we don’t have any data or any air cover from Google to prove to enterprises that the landscape has changed. 

I’d suggested that it would have been helpful to have insights from internal usability studies or some results from the Google Labs tests of AIOs to know search behavior is changing. The engineers seemed surprised to hear how universal the click losses have been. Ultimately, we were told, again, that things are moving so fast and are so volatile that it would have been difficult to provide any data or warnings up front to have helped our community through this process. However, there were allusions that there will be future releases that may help. Since that conversation, we’ve gotten a couple of articles on Search Central that allude to the improved quality of visits from search and direction to stop measuring clicks.

So, I’m not sure whether to say “I’m sorry” or “you’re welcome.” Accept whichever works for you.

However, it’s difficult to hear such things and then learn the next day from the Google Marketing Live event that [advertisers will have query-level data about AIOs](https://blog.google/products/ads-commerce/google-search-ai-brand-discovery/), but it is what it is. 

I also asked what they think our role should be in an agentic environment driven by reasoning, personal context, and DeepResearch. Aside from the standard “create great and unique and non-commodity content,” they said they weren’t sure.

And, that’s fine. We were in a similar position when RankBrain launched, and the party line was that Google didn’t know how their new stuff worked. It’s not like they were going to tell us to start using vector embeddings to understand the relevance of our content. It simply means it’s time to activate our community and get back to experimenting and learning. 

Unfortunately, I don’t know that everyone is going to make it through this era. The same way some of the last generation’s SEOs couldn’t survive the paradigm shift post-Panda and Penguin, I suspect some won’t cross the chasm into this next wave of search technology.

Those of us that will, we need to start from an understanding of how the technology works and then work our way back into what can be done strategically and tactically.

No present like the time…I took to figure this out for y’all.

## How AI Mode Works

I’m getting tired of watching people rewrite my posts in simpler ways, so we’ll start this with some prose as a simple overview of how AI Mode works. Then we’ll go through it in a more technical form with references to patents.

You can also use this NotebookLM file to get a podcast or ask your own questions to this post.

### Prose Version

You open Google and ask it a question. But what happens next doesn’t resemble search as you’ve known it. There are no blue underlines. Just a friendly, context-aware paragraph, already answering the next question before you think to ask it. Welcome to AI Mode.

### The Technical Version

Beneath the surface, what looks like a single reply is actually a matrixed ballet of machine cognition. First, your question is quietly reformulated into a constellation of other questions, some obvious, some implicit, some predictive. Google’s models “fan out” across this hidden web of synthetic queries, scanning not just for facts, but for ideas that can complete a “reasoning chain.”

Behind the scenes, the system isn’t just ranking content, it’s arguing with itself. It selects documents not because they won the SERP, but because they support a point in the machine’s obfuscated logic. Reasoning chains are like those old-school scratch-pad thoughts we all have while solving a problem and are now encoded into how answers are constructed. It’s not “What’s the best electric SUV?” It’s “What does ‘best’ mean to this user, right now, across these priorities?”

And, if that wasn’t enough, the models generating your answer aren’t monolithic. They’re task-specific, tuned and selected based on what kind of answer is needed. A summarizer. A comparer. A validator. It’s an ensemble cast with a rotating spotlight. Each contributes a line; a final model assembles the script.

All of this happens inside an invisible architecture powered by your past. Your clicks, your queries, your location, your Gmail threads are all boiled down into a vectorized version of… “you.”(You read that in Joe Goldberg’s voice, didn’t you?) A personalization layer that doesn’t just color the margins of the result, but warps the very selection of what qualifies as relevant.

And when the answer finally materializes, your webpages might be cited. They might not. Your content might appear not because you were optimized for the keyword, but because a single sentence happened to match a single sub-step in the machine’s invisible logic.

SEO spent the past twenty-five years preparing content to be parsed and presented based on how it ranks for a single query. Now, we’re engineering relevance to penetrate systems of reasoning across an array of queries.

Just mail my Pulitzer to the office.

Of course, Google has published some [high-level documentation on how AI Overviews and AI Mode work](https://search.google/pdf/google-about-AI-overviews-AI-Mode.pdf). But, you can see from your scrollbar that that is obviously not enough for me. So, in the spirit of the [late great Bill Slawski](http://www.seobythesea.com/), I’ve done a bit of my own research and uncovered some of Google’s patent applications that align with the functionality that we’re seeing.

The patent application for [“Search with Stateful Chat”](https://patents.google.com/patent/US20240289407A1/en) gives us a foundational understanding of how Google’s AI Mode functions. It marks a departure from classical search into a persistent, conversational model of information retrieval. The system understands you over time, draws from numerous synthetic queries, and stitches answers together using layered reasoning. Additionally, the [“Query Response from a Custom Corpus”](https://patents.google.com/patent/US20240362093A1/en) patent that fills in critical details about how responses are generated. It explains not just what the system knows about you, but how it selects which documents to pull from, how it filters them, and how it decides what to cite.

### AI Mode Employs Layered and Contextual Architecture

AI Mode operates as a multi-phase system built on top of Google’s classic index. Instead of treating each query in isolation, it maintains persistent user context by tracking your prior queries, locations, devices, and behavioral signals and turns each interaction into a vector embedding. This stateful context allows Google to reason about intent over time rather than just intent in the moment.

When a new query is entered, AI Mode kicks off a “query fan-out” process (don’t worry the deep-dive on that is coming) and generates dozens (or hundreds) of related, implied, and recent queries to uncover semantically relevant documents the user didn’t explicitly request. Each of these synthetic queries is used to retrieve documents from the index, which are then scored and ranked based on how well their vector embeddings align with both the explicit and hidden queries.

These documents form what the second patent calls a “custom corpus” or a narrow slice of the index that the system has determined is relevant for your query, at this moment, for you. This corpus is the foundation for the rest of the AI Mode response.

### AI Mode Uses Multi-Stage LLM Processing and Synthesis

Once the custom corpus is assembled, AI Mode invokes a set of specialized LLMs, each with a different function depending on the query classification and perceived user need. For example, some models may:

* Summarize comparative product reviews
* Translate or localize information across languages
* Extract and format structured data
* Apply reasoning across multiple documents

The patent lists some explicit assessments that are made about how to respond based on the understanding of the user’s information need:

* needs creative text generation
* needs creative media generation
* can benefit from ambient generative summarization
* can benefit from SRP summarization,
* would benefit from suggested next step query
* needs clarification
* do not interfere

From the patent’s description these align with LLMs, however, this is not a classic [Mixture of Experts (MoE) model](https://huggingface.co/blog/moe) with a shared routing layer. Instead, it’s a selective orchestration where specific LLMs are triggered based on context and intent. It’s closer in spirit to an intelligent middleware stack than a single monolithic model.

Although there is some discussion of generating hypothetical answers to compare the passages against, the system doesn’t generate responses out of thin air. Instead, as with all RAG pipelines, it extracts chunks from relevant documents, builds structured representations of that information, and synthesizes a coherent answer. Some chunks are cited; many are not. And as “Query response using a custom corpus” patent application describes, citation selection happens independently of document rank, based on how directly a passage supports the generated response.

#### AI Mode Leverages Dense Retrieval and Passage-Level Semantics

Though we’ve discussed embeddings multiple times, it’s worth saying that this entire pipeline runs on dense retrieval. Every query, subquery, document, and passage is converted into a vector embedding. Google, as Jori Ford reminded me that I’ve repeated ad nauseum for the past few years, calculates similarity between these vectors to determine what gets selected for synthesis. What matters is no longer just “ranking for the query,” but how well your document, or even an individual passage within it, aligns semantically with the hidden constellation of queries.

Additionally, Google’s retrieval pipeline no longer operates solely on static scoring functions like TF-IDF or BM25. While hybrid retrieval may still underpin initial candidate selection, the actual ranking and inclusion of content in generative answers increasingly depend on language model reasoning.

According to the [“Method for Text Ranking with Pairwise Ranking Prompting”](https://patents.google.com/patent/US20250124067A1/en) patent application, Google developed a novel system in which an LLM is prompted to compare two passages and determine which is more relevant to a user’s query. This process is repeated across many passage pairs, and the results are aggregated to form a ranked list.

Instead of assigning fixed similarity scores, the system asks: “Given this query, which of these two passages is better?” and lets the model reason it out. This represents a shift from absolute determinative relevance to relative, model-mediated probabilistic relevance. It aligns with AI Mode’s likely behavior, where:

* Dense retrieval surfaces a pool of candidate passages.
* Pairwise LLM prompting selects which passages are most valuable.
* The final synthesis model generates output based on the ranked results.

This has several strategic consequences:

* You’re not competing in isolation, you’re being compared directly to other sources chunk-by-chunk.
* The winner is chosen by a model capable of reasoning, not just counting tokens or (yikes) keyword density.
* Passage clarity, completeness, and semantic tightness become even more critical because your content must survive pairwise scrutiny.

The implication is clear: it’s not enough to rank somewhere for a topic. You must engineer passages that can outperform competing content head-to-head in LLM evaluations, not just semantic similarity. 

#### AI Mode Has Ambient Memory and Adaptive Interfaces

“Stateful chat” means Google accumulates an ambient memory of you over time just like James described for OpenAI. As described in the Search with stateful chat patent, these “memories” are likely aggregated embeddings representing past conversations, topics of interest, and search patterns. The interface itself adapts too, drawing from what we saw demonstrated in the Bespoke UI demo from last year.

It dynamically determines which elements (text, lists, carousels, charts) to display based on the information need and content structure. I highlighted this video in my talk at Semrush’s Spotlight conference last year as an indication of the future of search interfaces. When I first saw it, I knew we were in for something! Now we know that this functionality is powered by one of the downstream LLMs in the AI Mode pipeline.

#### AI Mode Does Personalization Through User Embeddings

A foundational innovation enabling AI Mode’s contextual awareness is the use of “user embedding” models as described in [User Embedding Models for Personalization of Sequence Processing Models](https://patents.google.com/patent/WO2025102041A1/en) patent application. This personalization mechanism allows Google to tailor AI Mode outputs to the individual user without retraining the underlying large language model. Instead, a persistent dense vector representation of the user is injected into the LLM’s inference pipeline to shape how it interprets and responds to each query.

This vector, called a user embedding, is generated from a user’s long-term behavioral signals: prior queries, click patterns, content interests, device interactions, and other usage signals across the Google ecosystem. Once computed, the user embedding acts as a form of latent identity, subtly influencing every stage of AI Mode’s reasoning process.

In practice, this embedding is introduced during:

* Query interpretation: altering how the model classifies intent,
* Synthetic query generation: shifting which fan-out queries are prioritized,
* Passage retrieval: re-ranking results based on individual affinity,
* Response synthesis: generating text or selecting formats (e.g., video, list, carousel) aligned with the user’s past preferences.

Importantly, this system allows for modular personalization: the same base model (e.g., Gemini) can serve billions of users while still producing individualized results in real time. It also introduces cross-surface consistency. The same user embedding could inform personalization across Search, Gemini, YouTube, Shopping, or Gmail-based recommendations. In fact, Tom Critchlow showcased on Twitter that he got the [same response in both AI Mode and Gemini](https://x.com/tomcritchlow/status/1925290349016023170).

No pun, but the implication is profound. AI Mode is no longer just intent-aware; it’s memory-aware. Two users asking the same query may see different citations or receive different answers, not because of ambiguity in the query, but because of who they are. That makes inclusion a function of both semantic relevance and profile alignment. That means logged-out rank tracking data is meaningless for AI Mode because responses can be 1:1.

#### The SEO Takeaway for AI Mode

AI Mode rewrites the rules. You’re no longer optimizing for a specific keyword or even a specific page. You’re optimizing for your content to be semantically relevant across dozens of hidden queries and passage-competitive within a custom corpus. Your ranking is [probabilistic](https://www.moveworks.com/us/en/resources/ai-terms-glossary/probabalistic), not [deterministic](https://en.wikipedia.org/wiki/Deterministic_system), and your presence in the result depends as much on embedding alignment as it does on authoritativeness or topical breadth.

To compete, you need to:

* Influence user search behavior through other branding channels
* Engineer content at the passage level for both semantic similarity and to be LLM-preferred
* Understand and anticipate synthetic query landscapes
* Optimize for semantic similarity and triple clarity
* Track rankings through profiles with curated user behaviors

This isn’t traditional SEO. This is Relevance Engineering (r19g). Visibility is a vector, and content is judged not only on what it says, but how deeply it aligns with what Google thinks the user meant.

### How Query Fan-Out Works

The query expansion technique Google refers to as “query fan-out” is fundamental to how AI Mode retrieves and selects content. Rather than issuing a single search, Google extrapolates the original query into a constellation of related subqueries in parallel. Some of these synthetic queries are directly derived, others inferred or synthesized from user context and intent. These queries span various semantic scopes and are used to pull candidate documents from the index. This enables Google to capture intent that the user didn’t, or couldn’t, explicitly express.

Google proudly discusses the concept from a high-level in the recent public documents, but the patent application [Systems and methods for prompt-based query generation for diverse retrieval](https://patents.google.com/patent/WO2024064249A1/en), offers a detailed blueprint of how query fan-out works. The process begins with a prompted expansion stage, where a LLM is used to generate multiple alternate queries from the original query. The model doesn’t hallucinate queries at random it is instructed with a structured prompt format that emphasizes:

* Intent diversity (e.g. comparative, exploratory, decision-making)
* Lexical variation (e.g. synonyms, paraphrasing)
* Entity-based reformulations (e.g. specific brands, features, topics)

#### Query Fan Out Synthetic Query Types

The query fan out process considers an array of different approaches to construct synthetic queries. Based on the various patents, the table below outlines the types of synthetic queries used:

| Synthetic Query Type    | Definition                                                                                                                                   | Trigger Condition                                                              | Role in AI Mode                                                                                    | Example (Base Query: “best electric SUV”)                 | Patent Source(s)                |
| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------- | --------------------------------------------------------- | ------------------------------- |
| Related Queries         | Queries that are semantically or categorically adjacent to the original query, often linked via entity relationships or taxonomy.            | Recognized co-occurrence patterns or topical proximity in the Knowledge Graph. | Expands retrieval scope to cover similar or overlapping domains of interest.                       | “top rated electric crossovers”                           | WO2024064249A1, US20240362093A1 |
| Implicit Queries        | Queries inferred from user intent, behavioral signals, or language model reasoning—what the user likely meant but didn’t explicitly say.     | LLM inference based on phrasing, ambiguity, and historical user behavior.      | Helps the model fulfill the deeper or unstated information need of the user.                       | “EVs with longest range”                                  | WO2024064249A1, US20240289407A1 |
| Comparative Queries     | Queries that compare products, entities, or options. Often synthesized when the user is making a choice or decision.                         | Classifier detects decision-making or ambiguity in original query.             | Triggers retrieval of structured or contrastive content for synthesis and re-ranking.              | “Rivian R1S vs. Tesla Model X”                            | WO2024064249A1, US20240362093A1 |
| Recent Queries          | Queries recently issued by the user, used to inform contextual understanding and query expansion in session-based or memory-informed search. | Prior queries in the session or search history retrieved via contextual layer. | Used to maintain conversational state and personalize fan-out expansion or synthesis.              | (Prior queries: “EV rebates in NY” → “best electric SUV”) | US20240289407A1                 |
| Personalized Queries    | Queries aligned to a specific user’s interests, location, or behavioral history (via embeddings).                                            | Retrieved from long-term user memory or injected user profile embeddings.      | Refines retrieval to reflect the unique context and past behavior of the individual user.          | “EVs with 3rd row seating near me”                        | WO2025102041A1, US20240289407A1 |
| Reformulation Queries   | Lexical or syntactic rewrites that maintain core intent but use different phrasing or vocabulary.                                            | Generated via prompt-based rewriting using LLMs (e.g., Gemini).                | Increases lexical diversity of query fan-out to capture alternate phrasings of the same intent.    | “which electric SUV is the best”                          | WO2024064249A1, WO2025102041A1  |
| Entity-Expanded Queries | Queries that substitute, narrow, or generalize based on entity relationships in the KG.                                                      | LLM crosswalks entity references to broader/narrower equivalents.              | Broadens or specifies scope using KG anchors—e.g., replacing “SUV” with specific models or brands. | “Model Y reviews”                                         | WO2024064249A1, US20240362093A1 |

Each of these is then routed through Google’s embedding-based retrieval system to locate relevant passages. What’s most important here is that ranking for the original query no longer guarantees visibility, because AI Mode is selecting content based on how well it aligns with one or more of the hidden fan-out queries, which, again, makes ranking in AI Mode a complex matrixed event.

#### Query Fan Out Filtering and Diversification

The Systems and methods for prompt-based query generation for diverse retrieval patent further outline a filtering mechanism to ensure the selected queries:

* Span multiple query categories (e.g., transactional, informational, hedonic)
* Return diverse content types (e.g., reviews, definitions, tutorials)
* Avoid overfitting to the same semantic zone (e.g., ensuring information diversity)

This helps Google build a more well-rounded and informative synthesis, pulling not just from the best-ranking document but from a custom corpus rich in contextual diversity. In other words, it’s not enough to just say what the competition is saying.

#### Query Fan-out Prompt-Based Chain of Thought

To improve quality and relevance, the synthetic query generation process may also include chain-of-thought prompting, where the LLM walks through reasoning steps like:

1. What the user is likely trying to achieve
2. What aspects of the original query are ambiguous or expandable
3. How to reframe the query to cover those needs

In other words, the LLM doesn’t just output alternate queries. It explains why each was generated, often using task-specific reasoning or structured intents (e.g., “Help the user compare brands,” “Find alternatives,” “Explore risks and benefits”).

#### The SEO Implication of Query Fan-Out

As I’ve learned more about query fan-out, I recognize that I wasn’t aware of it as a key aspect of AI Overviews. Early reports of AI Overviews pulling content from deep in the SERPs likely misunderstood what was happening. It’s probably not that Google’s AI was reaching far down the rankings for a single keyword; it was reaching across rankings for a different set of background queries entirely. So while SEOs are tracking position for \[best car insurance], Google may be selecting a passage based on how well it ranks for \[GEICO vs. Progressive comparison chart for new parents]. Based on [ZipTie’s latest data](https://ziptie.dev/blog/seo-still-matters-for-ai-search-engines/), ranking #1 for the core query only gives you a 25% chance at ranking in the AIO.

To surface in AI Mode, you must ensure:

* Your content ranks for multiple potential subqueries
* Your passages are semantically dense and well-aligned with diversified intents
* You engineer relevance not just for head terms, but for the expanded query space Google is quietly exploring in the background

### How Reasoning Works in Google LLMs

One of the defining features of Google’s AI Mode is its ability to reason across a corpus of documents to generate multi-faceted answers. The [“Instruction Fine-Tuning Machine-Learned Models Using Intermediate Reasoning Steps”](https://patents.google.com/patent/US20240256965A1/en) patent describes a system for constructing and using “reasoning chains.” These are structured sequences of intermediate inferences that connect user queries to generated responses in a logically coherent way. While this may not be the exact patent for how reasoning functions in AI Mode, it does give a sense of reasoning approaches that have informed iterations of Google’s models.

Rather than relying on end-to-end generation or selecting standalone answers, this system enables Google to:

1. Interpret the user’s intent and implicit needs
2. Formulate intermediate reasoning steps (e.g., “the user wants an SUV suitable for long commutes, so prioritize range and comfort”)
3. Retrieve or synthesize content for each step
4. Validate the final output against the logic of those steps

These reasoning chains may be segmented into the following groups:

* In-band – Steps generated as part of the LLM’s main output stream (e.g., via chain-of-thought prompting)
* Out-of-band – Steps created and refined separately from the final answer, then used to guide or filter that response
* Hybrid – Steps used for query expansion, document filtering, synthesis structuring, and validation at different points in the pipeline.

This is a dramatically different operation from what we are historically used to in SEO. The job now needs to include tactics to remain relevant throughout all these reasoning steps.

#### How Reasoning Works in the AI Mode Pipeline

Contextualizing everything we’ve learned with the steps in the Search with stateful chat patent application, we can get a sense of how and where reasoning is applied.
